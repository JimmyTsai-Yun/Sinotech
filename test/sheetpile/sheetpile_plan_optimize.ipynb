{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "import easyocr\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input \n",
    "annotation_pdf_path = \"C:\\\\Users\\\\jimmy\\\\Desktop\\\\sinotech2\\\\data\\\\pdf\\\\SheetPile\\\\plan_WordAndWall2.pdf\"\n",
    "wall_pdf_path = \"C:\\\\Users\\\\jimmy\\\\Desktop\\\\sinotech2\\\\data\\\\pdf\\\\SheetPile\\\\plan_OnlyWall2.pdf\"\n",
    "\n",
    "annotation_img_path = \"./plan_WordAndWall2.jpg\"\n",
    "wall_img_path = \"./plan_OnlyWall2.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF 解法\n",
    "* Input 一張標註圖，一張鋼板樁圖 \n",
    "##### 流程\n",
    "1. Annotation image preprocessing\n",
    "2. Arrow detection\n",
    "3. Line detection\n",
    "4. Arrow -> Line pairing\n",
    "5. key word extraction pair with (arrow-line)\n",
    "6. wall image preprocessing\n",
    "7. Line detection\n",
    "8. Line grouping according to the y axis\n",
    "9. Segment the main wall line according to the annotation detection result\n",
    "10. other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Annotation image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "# pdf to image\n",
    "annotation_images = convert_from_path(annotation_pdf_path, dpi=210)\n",
    "\n",
    "cv2_annotation_image = cv2.cvtColor(np.asarray(annotation_images[0]), cv2.COLOR_RGB2BGR)\n",
    "gray_annotation_img = cv2.cvtColor(cv2_annotation_image, cv2.COLOR_BGR2GRAY)\n",
    "_, th_annotation_img = cv2.threshold (gray_annotation_img, 240, 255, 0)\n",
    "GaussianBlur_annotation_img = cv2.GaussianBlur(th_annotation_img, (5, 5), 0)\n",
    "\n",
    "cv2.imwrite(annotation_img_path, GaussianBlur_annotation_img)\n",
    "\n",
    "# ocr\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "raw_annotation_img = cv2.imread(annotation_img_path)\n",
    "\n",
    "bounds = reader.readtext(raw_annotation_img, detail=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image preprocessing\n",
    "raw_annotation_img = cv2.imread(annotation_img_path)\n",
    "\n",
    "gray_annotation_img = cv2.cvtColor(raw_annotation_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, th_annotation_img = cv2.threshold(gray_annotation_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "filterSize = (2,2) # (4,4)for dpi = 300, (2,2) for dpi = 200 (6,6) for filter dpi=300\n",
    "bhat_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, filterSize)\n",
    "\n",
    "bhat_annotation_img = cv2.morphologyEx(th_annotation_img, cv2.MORPH_BLACKHAT, bhat_kernel)\n",
    "\n",
    "invers_annotation_img = cv2.bitwise_not(th_annotation_img)\n",
    "sub_annotation_img = invers_annotation_img - bhat_annotation_img\n",
    "\n",
    "erode_kernel = np.ones((4, 4))\n",
    "erode_annotation_img = cv2.erode(sub_annotation_img, erode_kernel, iterations=1)\n",
    "\n",
    "# turn the pixel to 0 if it is in the bounds\n",
    "for i in range(len(bounds)):\n",
    "    x1, y1 = bounds[i][0][0]\n",
    "    x2, y2 = bounds[i][0][2]\n",
    "    erode_annotation_img[y1:y2, x1:x2] = 0\n",
    "\n",
    "dilate_kernel = np.ones((5, 5))\n",
    "dilate_annotation_img = cv2.dilate(erode_annotation_img, dilate_kernel, iterations=2)\n",
    "\n",
    "cv2.imwrite(\"dilate.jpg\", dilate_annotation_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Arrow detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_contours, _ = cv2.findContours(dilate_annotation_img, cv2.RETR_LIST , cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# seperate two type of arrow\n",
    "hori_arrow = []\n",
    "vert_arrow = []\n",
    "for i in range(len(arrow_contours)):\n",
    "  x,y,w,h = cv2.boundingRect(arrow_contours[i])\n",
    "  if w > h:\n",
    "    hori_arrow.append(i)\n",
    "  else:\n",
    "    vert_arrow.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw img to check if the arrow is successfully seprated into hor and ver\n",
    "img = cv2.imread(\"plan_WordAndWall2.jpg\")\n",
    "for i in hori_arrow:\n",
    "  x,y,w,h = cv2.boundingRect(arrow_contours[i])\n",
    "  cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),1)\n",
    "for i in vert_arrow:\n",
    "  x,y,w,h = cv2.boundingRect(arrow_contours[i])\n",
    "  cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "\n",
    "cv2.imwrite(\"arrow.jpg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Line detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def FLD(image,drawimage, draw=False):\n",
    "    # Create default Fast Line Detector class\n",
    "    fld = cv2.ximgproc.createFastLineDetector()\n",
    "    # Get line vectors from the image\n",
    "    lines = fld.detect(image)\n",
    "\n",
    "    # Draw lines on the image\n",
    "    line_on_image = fld.drawSegments(drawimage, lines)\n",
    "\n",
    "    if draw:\n",
    "        cv2.imwrite('line_on_image.png', line_on_image)\n",
    "\n",
    "    return lines\n",
    "\n",
    "def for_hori(bl, tr, p, thr) :\n",
    "   if (p[0] > bl[0]-thr and p[0] < tr[0]+thr and p[1] > bl[1] and p[1] < tr[1]) :\n",
    "      return True\n",
    "   else :\n",
    "      return False\n",
    "\n",
    "def for_vert(bl, tr, p, thr) :\n",
    "   if (p[0] > bl[0] and p[0] < tr[0] and p[1] > bl[1]-thr and p[1] < tr[1]+thr) :\n",
    "      return True\n",
    "   else :\n",
    "      return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line thinning\n",
    "thinned_annotation_img = cv2.ximgproc.thinning(bhat_annotation_img)\n",
    "\n",
    "# get the lines\n",
    "annotation_img_lines = FLD(thinned_annotation_img, copy.deepcopy(raw_annotation_img), draw=False)\n",
    "\n",
    "annotation_df = pd.DataFrame(columns = ['left_arrow', 'right_arrow'],index = range(len(annotation_img_lines)))\n",
    "\n",
    "# 讓每條線都是頭的座標比較小\n",
    "check = []\n",
    "for i in range(len(annotation_img_lines)):\n",
    "  for j in range(len(annotation_img_lines)-(i+1)):\n",
    "    line1 = annotation_img_lines[i]\n",
    "    line2 = annotation_img_lines[i+j+1]\n",
    "    # line1 is correct\n",
    "    if float(line1[0,0]) < float(line1[0,2]) or float(line1[0,1] < float(line1[0,3])):\n",
    "      if ((float(line1[0,0])-float(line2[0,2]))**2 + (float(line1[0,1])-float(line2[0,3]))**2)**(1/2) < 10 and ((float(line1[0,2])-float(line2[0,0]))**2 + (float(line1[0,3])-float(line2[0,1]))**2)**(1/2) < 3:\n",
    "        check.append(i+j+1)\n",
    "    # line1 is not correct\n",
    "    else:\n",
    "      if ((float(line1[0,0])-float(line2[0,2]))**2 + (float(line1[0,1])-float(line2[0,3]))**2)**(1/2) < 10 and ((float(line1[0,2])-float(line2[0,0]))**2 + (float(line1[0,3])-float(line2[0,1]))**2)**(1/2) < 3:\n",
    "        check.append(i)\n",
    "\n",
    "# seperate two type of line\n",
    "hori_line = []\n",
    "vert_line = []\n",
    "for k, line in enumerate(annotation_img_lines):\n",
    "  if k  not in check:\n",
    "    if ((float(line[0,0])-float(line[0,2]))**2 + (float(line[0,1])-float(line[0,3]))**2)**(1/2) > 30:\n",
    "      if (float(line[0,0])-float(line[0,2]))**2 > (float(line[0,1])-float(line[0,3]))**2:\n",
    "        hori_line.append(k)\n",
    "      else:\n",
    "        vert_line.append(k)\n",
    "\n",
    "# remove the line that the head coordinate is smaller than the tail coordinate\n",
    "for i in hori_line:\n",
    "    if float(annotation_img_lines[i][0,0]) > float(annotation_img_lines[i][0,2]):\n",
    "        hori_line.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[248, 249, 250, 251, 252, 253, 937, 938, 939]\n"
     ]
    }
   ],
   "source": [
    "print(hori_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Arrow -> Line pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def arrow_line_pairing(hori_line, hori_arrow, vert_line, vert_arrow, arrow_contours, annotation_img_lines, df, raw_img, draw=False):\n",
    "    count = 0\n",
    "    for i in hori_line:\n",
    "        line = annotation_img_lines[i]\n",
    "        p1 = (int(line[0,0]),int(line[0,1]))\n",
    "        p2 = (int(line[0,2]),int(line[0,3]))\n",
    "        for j in hori_arrow:\n",
    "            x,y,w,h = cv2.boundingRect(arrow_contours[j])\n",
    "            b1 = (x,y)\n",
    "            tr = (x+w, y+h)\n",
    "            # check left side\n",
    "            if for_hori(b1, tr, p1, 2):\n",
    "                count+=1\n",
    "                cv2.line(raw_img, p1, p2, [0, 0, 255], 1) \n",
    "                cv2.rectangle(raw_img,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "                df.loc[i,'left_arrow'] = j\n",
    "            # check right side\n",
    "            elif for_hori(b1, tr, p2, 3):\n",
    "                df.loc[i,'right_arrow'] = j\n",
    "                cv2.line(raw_img, p1, p2, [0, 0, 255], 1) \n",
    "                cv2.rectangle(raw_img,(x,y),(x+w,y+h),[255,0,0],1)\n",
    "\n",
    "    for i in vert_line:\n",
    "        line = annotation_img_lines[i]\n",
    "        p1 = (int(line[0,0]),int(line[0,1]))\n",
    "        p2 = (int(line[0,2]),int(line[0,3]))\n",
    "        for j in vert_arrow:\n",
    "            x,y,w,h = cv2.boundingRect(arrow_contours[j])\n",
    "            b1 = (x,y)\n",
    "            tr = (x+w, y+h)\n",
    "            # check left side\n",
    "            if for_vert(b1, tr, p1, 3):\n",
    "                df.iat[i,0]= j\n",
    "                cv2.line(raw_img, p1, p2, [0, 0, 255], 1) \n",
    "                cv2.rectangle(raw_img,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "            # check right side\n",
    "            elif for_vert(b1, tr, p2, 3):\n",
    "                df.iat[i,1]= j\n",
    "                cv2.line(raw_img, p1, p2, [0, 0, 255], 1) \n",
    "                cv2.rectangle(raw_img,(x,y),(x+w,y+h),[255,0,0],1)\n",
    "    if draw:\n",
    "        cv2.imwrite(\"plan_pair.jpg\", raw_img)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df = arrow_line_pairing(hori_line, hori_arrow, vert_line, vert_arrow, arrow_contours, annotation_img_lines, annotation_df, copy.deepcopy(raw_annotation_img), draw=False)\n",
    "annotation_df_all = annotation_df.dropna(axis='index', how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    left_arrow right_arrow\n",
      "248         11         NaN\n",
      "250        NaN          10\n",
      "251          9           8\n",
      "252          7           6\n",
      "937          5           4\n",
      "938          3           2\n",
      "939          1           0\n"
     ]
    }
   ],
   "source": [
    "print(annotation_df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. key word extraction pair with (arrow-line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_array = []\n",
    "key_word = 'SHEET PILE'\n",
    "\n",
    "for j, bound in enumerate(bounds):\n",
    "  bound = list(bound)\n",
    "  if (bound[1].upper()).find(key_word) > -1:\n",
    "    dis = 1000000\n",
    "    middle_point = [(bounds[j][0][0][0] + bounds[j][0][2][0]) // 2,(bounds[j][0][0][1] + bounds[j][0][2][1]) // 2]\n",
    "    check: int = 0\n",
    "    for k in range(len(annotation_df_all)):\n",
    "      line = annotation_img_lines[annotation_df_all.index[k]]\n",
    "      p_mean = [ (int(line[0,0]) + int(line[0,2])) //2 , (int(line[0,1]) + int(line[0,3])) //2 ]\n",
    "      if ((middle_point[0]-p_mean[0])**2 + (middle_point[1]-p_mean[1])**2) < dis:\n",
    "        dis = (middle_point[0]-p_mean[0])**2 + (middle_point[1]-p_mean[1])**2\n",
    "        check = annotation_df_all.index[k]\n",
    "    pair_array.append([j,check])\n",
    "\n",
    "# 整理成dataframe\n",
    "index = list(range(len(pair_array)))\n",
    "\n",
    "pair_df = pd.DataFrame(np.arange((len(pair_array))*5).reshape(len(pair_array),5), columns=['head_x', 'head_y', 'end_x', 'end_y', 'ocr'],\n",
    "                  index=index)\n",
    "wall_type = []\n",
    "\n",
    "for i in pair_array:\n",
    "  indexx = i[0]\n",
    "  string = bounds[indexx][1]\n",
    "  wall_type.append(string)\n",
    "\n",
    "for index,i in enumerate(pair_array): \n",
    "  text_index = i[0]\n",
    "  line_index = i[1]\n",
    "  line = annotation_img_lines[line_index]\n",
    "  # if df index have left arrow use left arrow's x cooridnate\n",
    "  if not pd.isnull(annotation_df_all.loc[line_index,'left_arrow']):\n",
    "    x,y,w,h = cv2.boundingRect(arrow_contours[int(annotation_df_all.loc[line_index,'left_arrow'])])\n",
    "    pair_df.iloc[index,0] = x \n",
    "  else:\n",
    "    pair_df.iloc[index,0] = min(int(line[0,0]), int(line[0,2]))\n",
    "  pair_df.iloc[index,1] = min(int(line[0,1]), int(line[0,3]))\n",
    "  \n",
    "  # if df index have right arrow use right arrow's x cooridnate\n",
    "  if not pd.isnull(annotation_df_all.loc[line_index,'right_arrow']):\n",
    "    x,y,w,h = cv2.boundingRect(arrow_contours[int(annotation_df_all.loc[line_index,'right_arrow'])])\n",
    "    pair_df.iloc[index,2] = x + w\n",
    "  else:\n",
    "    pair_df.iloc[index,2] = max(int(line[0,0]), int(line[0,2]))\n",
    "  pair_df.iloc[index,3] = max(int(line[0,3]), int(line[0,1]))\n",
    "\n",
    "  pair_df.iloc[index,4] = wall_type[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   head_x  head_y  end_x  end_y                        ocr\n",
      "0    1194     722   1410    722  SP-IlI SHEET PILE (L=l6m)\n",
      "1    1415     722   3062    722   SP-IIlSHEET PILE (L=13m)\n",
      "2    3069     722   4716    722   SP-III SHEET PILE (L=9m)\n",
      "3     586    1658   1411   1658  SP-Ill SHEET PILE (L=l6m)\n",
      "4    1412    1658   3065   1658  SP-Ili SHEET PILE (L=l3m)\n",
      "5    3067    1658   4718   1658   SP-Iii SHEET PILE (L=9m)\n"
     ]
    }
   ],
   "source": [
    "print(pair_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. wall image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_images = convert_from_path(wall_pdf_path, dpi=210)\n",
    "cv2_wall_image = cv2.cvtColor(np.asarray(wall_images[0]), cv2.COLOR_RGB2BGR)\n",
    "gray_wall_img = cv2.cvtColor(cv2_wall_image, cv2.COLOR_BGR2GRAY)\n",
    "_, th_wall_img = cv2.threshold (gray_wall_img, 240, 255, 0)\n",
    "GaussianBlur_wall_img = cv2.GaussianBlur(th_wall_img, (5, 5), 0)\n",
    "\n",
    "cv2.imwrite(wall_img_path, GaussianBlur_wall_img)\n",
    "\n",
    "raw_wall_img = cv2.imread(wall_img_path)\n",
    "\n",
    "gray_wall_img = cv2.cvtColor(raw_wall_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret, th_wall_img = cv2.threshold(gray_wall_img , 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "bhat_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\n",
    "bhat_wall_img = cv2.morphologyEx(th_wall_img, cv2.MORPH_BLACKHAT, bhat_kernel)\n",
    "\n",
    "invers_wall_img = cv2.bitwise_not(th_wall_img)\n",
    "sub_wall_img = cv2.subtract(invers_wall_img, bhat_wall_img)\n",
    "\n",
    "erode_kernel = np.ones((1, 1))\n",
    "erode_wall_img = cv2.erode(sub_wall_img, erode_kernel, iterations=2)\n",
    "\n",
    "dilate_kernel = np.ones((6, 6))\n",
    "dilate_wall_img = cv2.dilate(erode_wall_img, dilate_kernel, iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Line detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line thinning\n",
    "thinned_wall_img = cv2.ximgproc.thinning(bhat_wall_img)\n",
    "# get all the lines in the image\n",
    "wall_img_lines = FLD(thinned_wall_img, copy.deepcopy(raw_wall_img), draw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Line grouping according to the y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def group_by_y_and_connect_extremes(lines, y_threshold=100):\n",
    "    \"\"\"Group lines based on y-coordinate and connect the leftmost and rightmost lines in each group.\"\"\"\n",
    "    # Sort lines by the average y-coordinate\n",
    "    lines.sort(key=lambda x: (x[1] + x[3]) / 2)\n",
    "\n",
    "    # Group lines by y-coordinate with a difference threshold\n",
    "    grouped_lines = []\n",
    "    current_group = [lines[0]]\n",
    "\n",
    "    for line in lines[1:]:\n",
    "        if abs(((line[1] + line[3]) / 2) - ((current_group[-1][1] + current_group[-1][3]) / 2)) > y_threshold:\n",
    "            grouped_lines.append(current_group)\n",
    "            current_group = [line]\n",
    "        else:\n",
    "            current_group.append(line)\n",
    "    \n",
    "    if current_group:\n",
    "        grouped_lines.append(current_group)\n",
    "\n",
    "    # For each group, find the leftmost and rightmost line and connect them\n",
    "    connected_lines = []\n",
    "    for group in grouped_lines:\n",
    "        if len(group) == 1:\n",
    "            connected_lines.append((group[0][0], group[0][1], group[0][2], group[0][3]))\n",
    "            continue\n",
    "        # Find the leftmost and rightmost lines\n",
    "        leftmost = min(group, key=lambda x: min(x[0], x[2]))\n",
    "        rightmost = max(group, key=lambda x: max(x[0], x[2]))\n",
    "        # Connect the leftmost start to the rightmost end\n",
    "        connected_lines.append((min(leftmost[0], leftmost[2]), \n",
    "                                (leftmost[1] + leftmost[3]) / 2, \n",
    "                                max(rightmost[0], rightmost[2]), \n",
    "                                (rightmost[1] + rightmost[3]) / 2))\n",
    "\n",
    "    return connected_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_list = []\n",
    "for line in wall_img_lines:\n",
    "    x1, y1, x2, y2 = line[0]\n",
    "    lines_list.append([x1, y1, x2, y2])\n",
    "\n",
    "connected_lines_sample = group_by_y_and_connect_extremes(lines_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Segment the main wall line according to the annotation detection result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def segment_main_lines_with_labels(connected_lines_by_y, ocr_data):\n",
    "\n",
    "    # Extract the bounds for the upper and lower lines\n",
    "    upper_bounds = (connected_lines_by_y[0][0], connected_lines_by_y[0][2])\n",
    "    lower_bounds = (connected_lines_by_y[1][0], connected_lines_by_y[1][2])\n",
    "\n",
    "    # Determine the mid y-coordinates of upper and lower lines\n",
    "    upper_mid_y = (connected_lines_by_y[0][1] + connected_lines_by_y[0][3]) / 2\n",
    "    lower_mid_y = (connected_lines_by_y[1][1] + connected_lines_by_y[1][3]) / 2\n",
    "    \n",
    "    # Function to determine the closest main line based on y-coordinate\n",
    "    def closest_main_line(y_coord):\n",
    "        if abs(y_coord - upper_mid_y) < abs(y_coord - lower_mid_y):\n",
    "            return 'upper'\n",
    "        else:\n",
    "            return 'lower'\n",
    "    \n",
    "    # Assign lines to upper or lower based on y-coordinate\n",
    "    ocr_data['line_group'] = ocr_data.apply(lambda row: closest_main_line((row['head_y'] + row['end_y']) / 2), axis=1)\n",
    "\n",
    "    # Extract segments for upper and lower lines\n",
    "    segments_upper = ocr_data[ocr_data['line_group'] == 'upper'][['head_x', 'end_x', 'ocr']]\n",
    "    segments_lower = ocr_data[ocr_data['line_group'] == 'lower'][['head_x', 'end_x', 'ocr']]\n",
    "\n",
    "    # Segmenting logic as previously defined\n",
    "    def correct_segment_main_line(bounds, segments):\n",
    "        line_segments = []\n",
    "        start, end = bounds\n",
    "        sorted_segments = sorted(segments, key=lambda x: x['head_x'])\n",
    "\n",
    "        # Initialize the previous end to the start of the main line\n",
    "        prev_end = start\n",
    "\n",
    "        # Iterate through sorted segments and ensure no overlap\n",
    "        for segment in sorted_segments:\n",
    "            current_start = max(prev_end, segment['head_x'])  # Start at the greater of previous end or current head_x\n",
    "            current_end = segment['end_x']\n",
    "            \n",
    "            # If there's a gap between the previous end and the current start, fill it with the previous segment's label\n",
    "            if current_start > prev_end:\n",
    "                line_segments.append((prev_end, current_start, segment['ocr']))  # Use the first segment's label for any initial gap\n",
    "            \n",
    "            # Append the current segment\n",
    "            line_segments.append((current_start, current_end, segment['ocr']))\n",
    "            prev_end = current_end  # Update the previous end\n",
    "\n",
    "        # Handle any remaining portion of the main line after the last segment\n",
    "        if prev_end < end:\n",
    "            line_segments.append((prev_end, end, sorted_segments[-1]['ocr']))  # Use the last segment's label for the final gap\n",
    "\n",
    "        return line_segments\n",
    "\n",
    "    # Segmenting the upper and lower lines\n",
    "    segmented_upper = correct_segment_main_line(upper_bounds, segments_upper.to_dict('records'))\n",
    "    segmented_lower = correct_segment_main_line(lower_bounds, segments_lower.to_dict('records'))\n",
    "\n",
    "    # Combine and return segmented data\n",
    "    segmented_data = {'upper': segmented_upper, 'lower': segmented_lower}\n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_data = segment_main_lines_with_labels(connected_lines_sample, pair_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590.86334, 1194, 'SP-IlI SHEET PILE (L=l6m)')\n",
      "(1194, 1410, 'SP-IlI SHEET PILE (L=l6m)')\n",
      "(1410, 1415, 'SP-IIlSHEET PILE (L=13m)')\n",
      "(1415, 3062, 'SP-IIlSHEET PILE (L=13m)')\n",
      "(3062, 3069, 'SP-III SHEET PILE (L=9m)')\n",
      "(3069, 4716, 'SP-III SHEET PILE (L=9m)')\n",
      "-----------------------------------\n",
      "(603.79846, 1411, 'SP-Ill SHEET PILE (L=l6m)')\n",
      "(1411, 1412, 'SP-Ili SHEET PILE (L=l3m)')\n",
      "(1412, 3065, 'SP-Ili SHEET PILE (L=l3m)')\n",
      "(3065, 3067, 'SP-Iii SHEET PILE (L=9m)')\n",
      "(3067, 4718, 'SP-Iii SHEET PILE (L=9m)')\n"
     ]
    }
   ],
   "source": [
    "for i in segmented_data['upper']:\n",
    "    print(i)\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "for i in segmented_data['lower']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "plan_wall_img= cv2.imread(\"plan_WordAndWall2.jpg\")\n",
    "\n",
    "# Extract the length value from the OCR text and create a color mapping based on unique length values\n",
    "length_pattern = re.compile(r\"L=(\\d+)m\")\n",
    "\n",
    "def extract_length(text):\n",
    "    match = length_pattern.search(text.replace('l', '1'))  # Replace possible 'l' with '1' for consistency\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "df_copy = pair_df.copy()\n",
    "\n",
    "# Update the segments data to include length values\n",
    "df_copy['length'] = df_copy['ocr'].apply(extract_length)\n",
    "unique_lengths = df_copy['length'].unique()\n",
    "# Plotting segmented lines with their labels\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255), (255, 0, 255)]\n",
    "color_map = {length: color for length, color in zip(unique_lengths, colors[:len(unique_lengths)])}\n",
    "\n",
    "# Visualization function updated to use length-based colors\n",
    "def plot_segmented_lines_by_length(segments, connected_lines_sample):\n",
    "    for keys in segments.keys():\n",
    "        for segment in segments[keys]:\n",
    "            if keys == 'upper':\n",
    "                y_level = connected_lines_sample[0][1]\n",
    "            else:\n",
    "                y_level = connected_lines_sample[1][1]\n",
    "            length = segment[2].split(' ')[-1]\n",
    "            color = color_map[extract_length(segment[2])]\n",
    "            cv2.line(plan_wall_img, (int(segment[0]), int(y_level)), (int(segment[1]), int(y_level)), color, 2)\n",
    "            mid_x = int((segment[0] + segment[1]) // 2)\n",
    "            cv2.putText(plan_wall_img, length, (mid_x, int(y_level)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    cv2.imwrite('plan_OnlyWall2_segmented3333.jpg', plan_wall_img)\n",
    "\n",
    "plot_segmented_lines_by_length(segmented_data, connected_lines_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.98387096774194\n"
     ]
    }
   ],
   "source": [
    "dis1 = (3062 - 1410)*15/620\n",
    "dis2 = (3065 - 1411)*15/620\n",
    "print(dis1+dis2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinotech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
